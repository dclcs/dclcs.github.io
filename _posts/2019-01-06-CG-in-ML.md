---
layout: post
title: "图形深度学习 ML BASICS(I)"
description: "CG"
category: CG
tags: [CG]
---

{% include JB/setup %}


### Machine Learning
`机器学习`是计算机科学的一个领域，这个领域使用<b>统计学</b>的方法让计算机系统使用<b>数据</b>去<b>学习</b>，而不是提前被设计好的。
!['ml-define'](../images\ml-define.png)
#### ML种类
- <b>监督类</b>
    - 分类问题
        - Digit Recognition
        - Spam Detection
        - Face detection
    - 回归问题
        - Human Face/Pose Estimation
        - Model Estimation
    - Data consolidation(数据合并？？)
- <b>非监督</b>
    - 聚类（Clustering）
        - Group Points According to X
        !['clu'](../images\clus.png)
        - Image Segmentation using NCuts
        !['clu'](../images\ncuts.png)
    - 降维问题（Dimensionality Reduction）
        - 多种学习融合（Manifold Learning）
- <b>弱监督/半监督</b>
    - 一些数据采用监督， 一些数据非监督
- <b>强化学习</b>
    - Supervision: sparse reward for a sequence of decisions（监督：对一系列决策的稀疏奖励）
<hr/>
<b>PS:</b>
    Segmentation + Classification in Real Images

!['se+clas'](../images\seg&class.png)
<b>评价标准</b>: Confusion matrix, ROC curve, precision, recall, etc.
<hr/>


### Learning a Function

$y = f_w(x)$, 其中$y$是预测（prediction）;$f$是预测方法(method)；$w$是参数(parameters);$x$是输入(input).

#### Learning a Linear Separator/Classifier

!['linear'](../images\linear.png)
!['sh'](../images\sh.png)
##### $y = f(w_1x_1 + w_2x_2) = \mathcal H(w_1x_1 + w_2x_2)$
其中$\mathcal H$fixed non-linearity 并且$w_1 ,w_2$是通过学习得到的。

#### Combining Simple Functions/Classifiers
!['multi'](../images\multi.png)
!['multi'](../images\multi2.png)


### Regression
#### 1. Least Squares fitting
- Assumption:Linear Function

$$y = f_\mathbf w(\mathbf x) = f(\mathbf x, \mathbf w) = \mathbf w^T\mathbf w \\ \mathbf w^T\mathbf w = \langle \mathbf w^T,\mathbf w \rangle = \sum_{d=1}^D \mathbf w_d \mathbf x_d \\ \bf x \in \mathbb R^D, \mathbf w \in \mathbb R_D$$

- Reminder: Linear Classifier
!['rs'](../images\rs.png)
    - Sum of Square Errors (MSE without the mean):\
        $y^i = \mathbf w^T\mathbf x^i + \epsilon^i$
        
        loss function:Sum of Square Errors
        
        $L(\mathbf w) = \sum_{i=1}^N(\epsilon^i)^2$

        展开：
        $L(w_0, w_2) = \sum_{i=1}^N[y_i - (w_0x^i_0 + w_1x^i_1)]^2$
    - Q:what is the best (or least bad) value of w?:\
        $\bf y = \bf X\bf w + \bf \epsilon$

        $L(\bf w) = \epsilon^T\epsilon$

        $minmize L(\bf w)$

        $L(\bf w) = (\bf y - \bf X\bf w)^T(\bf y - \bf X\bf w) = \mid\mid\bf y - \bf X\bf w\mid\mid^2$

        $\nabla L = 2\bf X^T(y - Xw) = 0 \Rightarrow w^* = (X^TX)^{-1}X^Ty$


