---
title: "图形深度学习 ML BASICS(II)"
author: dcl
categoies: CG
CreateTime: 2019-02-06 17:23:3 +0800
date: 2019-02-06 17:23:3 +0800

---
来自Siggraph

<!--more-->

##### 神经网络训练：旧和新的技巧
- Old
    - 反向传播
        - 训练目标
            - 我们的网络实施是一个参数化函数<br>$f_\theta: \mathbb X \to \mathbb Y$, $\widehat y = f(x;\theta)$
            - 在训练过程中，我们寻找一个参数最小化损失函数:${min}_\theta L_f(\theta)$
            - 例子：L2 regression loss
        - 梯度
            - 前向传播
            - 反向传播
    - 随机梯度、动量、权重衰减（`weight decay`）
        - 对于训练`N`个样本的目标函数：$L(\mathbf W) = \frac{1}{N}\sum^N_{i=1}l(y^i,\hat y^i) + \sum_l \lambda_l\sum_{k,m}(\mathbf W^l_{k,m})^2$
            - 其中，$l(y^i,\hat y^i)$为每一个样本的损失函数；$\sum_{k,m}(\mathbf W^l_{k,m})^2$为每一层的正规（`regularization`）。
            - 梯度下降：$\mathbf W_{t+1} = \mathbf W_{t} - \epsilon \nabla_{\mathbf W}L(\mathbf W_t)$
            - 第$(l, k, m)$项的梯度向量：$\frac{\partial L}{\partial \mathbf W^l_{k,m}} = \frac{1}{N} \sum_{i=1}^N\frac{\partial l(\mathbf y^i, \hat  \mathbf y^i)}{\partial \mathbf W_{k,m}^l} + 2\lambda_l \mathbf W^l_{k,m}$
                - 其中，$\frac{\partial l(\mathbf y^i, \hat  \mathbf y^i)}{\partial \mathbf W_{k,m}^l}$为第i个例子反向传播的参数梯度。
        - `SGD`的正则化：权重衰减（`weight decay`）
            - 梯度：
                - <b>Batch：</b>$[1...N]$
                - $\frac{\partial L}{\partial \mathbf W^l_{k,m}} = \frac{1}{N} \sum_{i=1}^N\frac{\partial l(\mathbf y^i, \hat  \mathbf y^i)}{\partial \mathbf W_{k,m}^l} + 2\lambda_l \mathbf W^l_{k,m}$
                - 噪声（'随机梯度'）
                    - <b>MiniBatch:</b>B个元素，从$[1,N]$中随机抽取B个样本
                    - $\frac{\partial L}{\partial \mathbf W^l_{k,m}} \cong \frac{1}{B} \sum_{i=1}^B\frac{\partial l(\mathbf y^{b(i)}, \hat  \mathbf y^{b(i)})}{\partial \mathbf W_{k,m}^l} + 2\lambda_l \mathbf W^l_{k,m}$
        - 学习率（`learning rate`）:
            - $\mathbf W_{t+1} = \mathbf W_{t} - \epsilon \nabla_{\mathbf W}L(\mathbf W_t)$
- New
    - Dropout
    - Relu
    - Batch Norm(alization), GroupNorm, Spectral Normalization
    - Res(idual)Net(work)

