---
title: 学习报告(week&nbsp;4)
date: 2018-11-16 10:53:00
description: Keep it SMPL&#58; Automatic Estimation of 3D Human Pose and Shape from a Single Image
categories:
 - weekly
tags: 
- note
---

## 学习报告(week&nbsp;4)

----------

### [Keep it SMPL&#58; Automatic Estimation of 3D Human Pose and Shape from a Single Image](https://arxiv.org/abs/1607.08128v1)

#### Getting Start

这篇文献介绍了一种新技术 **SMPLify**。通过使用 **SMPLify**，我们可以在不进行手工标注特征的情况下将一幅单纯 2D 的人体姿势图像转化为对应的 3D 模型。首先，作者使用了一个名为 **DeepCut** 的 CNN 来提取 2D 图像输入的特征关节点，但此时的关节点是缺乏深度信息的。作者接下来对上一步获得的 2D 关节点使用他们先前提出的 **SMPL** 模型来生成对应的 3D 模型，然后通过 Powell’s dogleg method(狗腿算法) 来优化 3D 模型的关节点，最后获得所需要的输出。 <br />

#### DeepCut

在正式介绍 **SMPLify** 的内容前，我想先简略地描述一下 **SMPLify** 所用到的两个重要的工具。DeepCut 是一个 CNN，它首先提取图片中的 body part candidates(人体部件候选区域)，然后令每一个候选区域对应一个关节点，令每一个关节点作为图中的一个节点。接下来，DeepCut 将属于同一个人的关节点归位一类，同时标记每一个节点属于哪一个人体部分。最后，分别将同一类中拥有不同标记的节点组合成一个人的姿态估计。 <br />

这种姿态估计的文章涉及到很多新的知识点，很难在一时半刻完全掌握 DeepCut 的内容，所有这里就暂时不进行过多的介绍了。在这篇文章中，我们只需要知道，DeepCut 可以提取出 2D 人像中的关节点，并对这些关节点计算出对应的自信度。实际上，目前已有名为 **Deepercut**，它貌似在效率与效果上都对 **DeepCut** 进行了改进，如果在 **SMPLify** 上应用 **Deepercut** 的话，或许能获得更好的效果。但考虑到我们项目的主要方向是 fashion style generator，在此就暂时不深究这方面的内容的(如果日后要寻求进一步的改善的话，**Deepercut** 技术或许是一个很好的 alternative)，我们只需要探讨如何把 **SMPLify** 应用到自己的项目中即可。 <br />

此处是一些相关的阅读材料：[DeepCut: Joint subset partition and labeling for multi person pose estimation](https://arxiv.org/abs/1511.06645)、[DeeperCut: A Deeper, Stronger, and Faster Multi-Person Pose Estimation Model](https://arxiv.org/abs/1605.03170)。 <br />

#### SMPL

**SMPL** 是一种基于参数化模型的人体建模方法，它由 Loper et al. 所提出。要彻底弄清楚 **SMPL** 的优势需要大量的背景知识，我们这里只需先了解它的一些基本信息。在 **SMPL** 中，一个 3D 模型可以被 β 与 θ 表示，其中，β 为 body shape(即人体的高矮胖瘦、头身比例等 10 个参数)，θ 为 body pose(即人体整体运动位姿和 24 个关节的相对角度，共 75 个参数)。 <br />

此处是一些相关的阅读材料：[SMPL: A Skinned Multi-Person Linear Model](http://files.is.tue.mpg.de/black/papers/SMPL2015.pdf)、[SMPL official website](http://smpl.is.tue.mpg.de)。 <br />

#### SMPLify

##### Introduction

**SMPLify** 在 2016 由 Bogo et al. 提出，它能将一张 2D 人像转化为对应的 3D 模型，并在包含众多复杂姿势的 Leeds Sports Pose Dataset 中取得了不错的成果，其官方网站为：[SMPLify official website](http://smplify.is.tuebingen.mpg.de)。 <br />

我们可以从一下的效果图中观察到，**SMPLify** 的确能在处理 2D 人像转化为对应的 3D 模型的问题上有优秀的表现，同时，它的最大优点在于不需要进行手动特征标注即可获得很好的转换效果(尽管如此，手动进行性别标注可以获得更好的效果)。 <br />

![](https://raw.githubusercontent.com/Eros-L/Eros-L.github.io/master/_posts/thesis/week4/smplify_example_result.png)

且外，文章也介绍了若干的 related works，结果是 **SMPLify** 在多个测试集下均能获得较好的结果，但我们无需对这些进行深入了解，所以就不再赘述了。 <br />

##### Method

首先，我们先通过一张图片来加深对 **SMPLify** 实现流程的初步认识： <br />

![](https://raw.githubusercontent.com/Eros-L/Eros-L.github.io/master/_posts/thesis/week4/smplify_overview.png)

**SMPLify** 先接收到一张 2D 的人像图片，除了性别信息外，我们不需要再提供其他的额外信息。接下来，我们使用之前提及过的 **DeepCut** 网络来获得 2D 人像的节点信息。之后，我们使用 **SMPL** 来生成对应的 3D 模型，并通过 Powell’s dogleg method 来最小化生成模型的节点与 2D 人像节点的误差，从而获得最优的模型。 <br />

**SMPL** 人体模型可表示为 *M*(*β*,*θ*,*γ*) 的形式，其中，*β* 为 body shape，*θ* 为 body pose，*γ* 为 translation(事实上，我们可以从后文中的值，这里的 translation 即为 camera translation)。需要注意的是，**DeepCut** 所输出的节点数目与 **SMPL** 输出的节点数目略有不同，作者这里将两种方法中相近的节点联系在了一起。 <br />

##### Approximating Bodies with Capsules

为了避免 interpenetration(即人体模型出现不自然的姿势，人的肢体不正常地相交在了一起)，我们往往需要进行额外的计算，而计算人体表面的 interpenetration 的开销是非常大的，作者因此使用了 capsule(胶囊体) 来近似人体模型的不同部分。在这里，作者使用了回归的方式完成从 body shape 与 body pose 到 capsule 的转换。 <br />

##### Objective Function

这部分属于文章的重点内容，我们首先介绍 Powell’s dogleg method 中用到的 Error 函数： <br />

![](https://raw.githubusercontent.com/Eros-L/Eros-L.github.io/master/_posts/thesis/week4/error.png)

在这条公式中，*β* 与 *θ* 已经解释过了，*Jest* 代表从 **DeepCut** 获得的 2D 节点，*K* 代表摄像机参数，四个不同 *λ* 代表各个子误差的权重。 <br />

![](https://raw.githubusercontent.com/Eros-L/Eros-L.github.io/master/_posts/thesis/week4/joint-based_error.png)

对于 joint-based error term，我们可以将其理解成直接计算对应节点间的误差。*J*(*β*)<sub>*i*</sub> 表示根据 body shape 所获得的 3D 骨骼节点位置的预测；*R*<sub>*θ*</sub>(*J*(*β*)<sub>*i*</sub>) 表示 3D 姿势节点，而 3D 姿势节点是通过 3D 骨骼节点进行变换获得的。在获得 3D 姿势节点后，我们使用 *Π*<sub>*K*</sub> 将其投影到 2D 上，当中的 *K* 可以近似理解为视角。然后，我们使用 Geman-McClure 范数来表示模型节点与实际 2D 节点的误差，并使用从 **DeepCut** 获得的节点的自信度 *w*<sub>*i*</sub> 作为节点误差的权值。最后，joint-based error term 等于每个节点误差的加权和。 <br />

![](https://raw.githubusercontent.com/Eros-L/Eros-L.github.io/master/_posts/thesis/week4/unnatural-pose_error.png)

上文也提到 interpenetration 的问题，而为了解决这个问题，我们需要专门为其计算一个 error term。这里的 *i* 代表与膝盖弯曲、肘部弯曲有关的姿势节点。这里的 *θ* 代表 rotation 的角度，正常情况下，*θ* 为负值，则 0 < exp(*θ*) < 1；若 *θ* 为正值，意味着发生了不自然的扭转，exp(*θ*) 一般将 >> 1。需要注意的是，*θ* 在不发生旋转时等于 0。 <br />

![](https://raw.githubusercontent.com/Eros-L/Eros-L.github.io/master/_posts/thesis/week4/favorable-pose_error.png)

为了进一步淘汰掉不正常的姿势，我们可以预先为模型设定一些 favorable pose(可以理解成较为常见的姿势)。作者在 **CMU marker dataset** 上通过 **MoSh** 来获得一系列的 **SMPL** 模型，然后用这些模型来建立若干个高斯分布。然后在误差项中，*g*<sub>*j*</sub> 代表 8 个高斯分布中的其中一个，*c* 是一个常量。为了节省计算量，作者仅对高斯映射过程中偏离最大的值取对数作为误差。 <br />

![](https://raw.githubusercontent.com/Eros-L/Eros-L.github.io/master/_posts/thesis/week4/capsule_error.png)

之前也有提到用 capsule 来近似人体部分，并借此避免 interpenetration 的问题，这里给出了其误差计算方法。我们进一步将 capsule 近似为 sphere，这里的 *C*(*θ*,*β*) 代表胶囊的中心坐标，*r*(*β*) 代表胶囊的半径，*σ*(*β*) 等于 *r*(*β*)/3。*I*(*i*) 则是对于第 *i* 个球体，与其不相容的球体的集合。这里比较的主要是两个球体的圆心距和半径和，比较这两者的大小可以获得两个球体的位置关系，从而达到惩罚 interpenetration 的作用。说实话，我这里不太明白 3D isotropic Gaussian 的作用，还需要进一步学习才能彻底理解这一步的操作。 <br />

![](https://raw.githubusercontent.com/Eros-L/Eros-L.github.io/master/_posts/thesis/week4/capsule_error.png)

最后，作者定义了一个 shape error term 来衡量 body shape 的误差。这里使用了 PCA(即 Principal Component Analysis，主成分分析，用以消除变量间的关联性) 的方法来获得 error term。

##### Optimization

由于拍照的角度是不确定的，输入的人像未必正对着拍照平面，因此，我们需要引入 *γ*(camera translation) 来进一步优化模型。首先，我们可以比较容易地从人像图片中获取到摄像机的大致焦距，然后使用相似三角形的方法计算出人体的大致深度(即根据 3D 模型躯干节点长度与 2D 躯干节点长度的比值估算深度)。然后在正式计算前，先使用 *E*<sub>*j*</sub> 对 *γ* 进行优化。 <br />

接下来，可以正式开始将 2D 人像转化为 3D 模型。在这个过程中，作者发现为 *λ*<sub>*θ*</sub> 与 *λ*<sub>*β*</sub> 设置较高的初始值并让它们逐渐衰减能有效避免结果陷入局部最优。 <br />

在最开始的时候，我们是无法得知人体的朝向的，所以当 2D 节点中的肩距小于某一个值时，我们可以认定当前人像并没有正对着镜头，我们尝试选择人体朝向，并筛选出较优的角度。 <br />

最后，使用 **Powell’s dogleg method** 获得一组最优的 3D 节点，整个过程仅需 1 分钟即可完成。 <br />

##### Evaluation

考虑到实际情况中，一幅 2D 图片很少会带有一个 ground truth 的 3D 模型，作者在此同时使用了 synthetic data 与 real data 来进行评估。

在 synthetic data 部分中，作者先从一系列 **SMPL** 模型中获取到 2D 节点，并对这些 synthetic data 添加随机的高斯噪声，然后以 synthetic data 为输入，通过上述方法对模型进行评估。且外，作者还假设已知 body pose，在此基础上尝试用尽量少的节点来表示 body shape。这部分的结果如下图所示： <br />

![](https://raw.githubusercontent.com/Eros-L/Eros-L.github.io/master/_posts/thesis/week4/synthetic_data_evaluation.png)

在 real data 部分中，作者分别在 *HumanEva-I.*、**Human3.6M.**、**Leeds Sports Dataset.** 训练集上与其他模型进行了对比评估。一系列的数据表明，**SMPLify** 要优于其他一些比较先进的模型的，这里只需留意一下数据即可。 <br />

![](https://raw.githubusercontent.com/Eros-L/Eros-L.github.io/master/_posts/thesis/week4/HumanEva-I_result.png)
<br />
![](https://raw.githubusercontent.com/Eros-L/Eros-L.github.io/master/_posts/thesis/week4/Human3-6M_result.png)

同时，作者也用实验数据证明了，multi-modal pose prior 与 interpenetration error term 能显著提升模型的表现。 <br />

![](https://raw.githubusercontent.com/Eros-L/Eros-L.github.io/master/_posts/thesis/week4/HumanEva-I_ablation_study.png)

接下来也附上一些视觉可见的结果来辅助证明 **SMPLify** 的优势。我们可以观察到，**SMPLify** 不仅在一些简单姿势上能获得明显优于其他模型的效果，**SMPLify** 在处理复杂的运动姿势时依然也能给出很好的表现。 <br />

![](https://raw.githubusercontent.com/Eros-L/Eros-L.github.io/master/_posts/thesis/week4/LSD_result.png)
<br />
![](https://raw.githubusercontent.com/Eros-L/Eros-L.github.io/master/_posts/thesis/week4/comparison.png)

而 **SMPLify** 也存在失败的情况，在处理 **Leeds Sports Dataset.** 时 **SMPLify** 也会生成错误的模型，在此可以简单对失效情况进行概括。可能失效的情况包括：不同四肢的高度重叠、在缺少深度信息情况下不同人物的重叠、无法正确分辨人物朝向。在这当中，我认为第一二种失效情况是比较难解决的；而相对的第三种情况可以通过在测量出的 2D 节点肩距小于某个值时，多尝试几组不同的可能人体朝向角度，而不是仅以 180° 作为步长。 <br />

![](https://raw.githubusercontent.com/Eros-L/Eros-L.github.io/master/_posts/thesis/week4/LSD_failure.png)

----------

### Conclusion

总的来说，**SMPLify** 能仅依靠单纯的 2D 图像输入获得不俗的 3D 建模结果。实际上，我可能不需要思考如何对 **SMPLify** 进行改进，而是去思考如何在自己的项目中应用 **SMPLify**。在我看来，目前的 image-based virtual try-on network 的最大问题在于无法处理复杂的姿势与大幅度的形变，且外，在结果上看，输出也只是单纯的风格迁移，并没能保留服装的一些三维信息。如果我们把 **SMPLify** 的输出作为 image-based virtual try-on network 输入的深度信息的话，我们或许可以获得更加理想的结果(这一步可以参考目前一些需要用到深度信息的试衣系统，这些系统通常有不错的效果，只是实用性不高，如果使用 **SMPLify** 的话，就有可能将一个需要深度测量的问题转化为一个 2D 图像处理问题)。 <br />
